{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from bs4 import NavigableString\n",
    "import re\n",
    "import string\n",
    "import nltk.tree\n",
    "import nltk.tokenize\n",
    "import nltk.corpus\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.tree import *\n",
    "import csv\n",
    "import pandas as pd\n",
    "import collections\n",
    "import urllib2\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def buildSlangDictionary():\n",
    "    alpha = list(map(chr, range(ord('a'), ord('z')+1)))\n",
    "    url = 'http://www.noslang.com/dictionary/'\n",
    "    headers = {\n",
    "        'User-Agent': \"Mozilla/5.0 (Windows NT 6.1; Win64; x64)\",\n",
    "        'Accept': \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "        'Accept-Language': \"de,en-US;q=0.7,en;q=0.3\",\n",
    "        'Accept-Charset': \"ISO-8859-1,utf-8;q=0.7,*;q=0.3\",\n",
    "        'Accept-Encoding': 'none',\n",
    "        'Connection': \"keep-alive\"\n",
    "    }\n",
    "    slang_dict = collections.defaultdict(str)\n",
    "    for letter in alpha:\n",
    "        url = url + letter + '/'\n",
    "        req = urllib2.Request(url, headers=headers)\n",
    "        response = urllib2.urlopen(req)\n",
    "        soup = BeautifulSoup(response)\n",
    "        abbr_list = []\n",
    "        for abbr in soup.findAll('abbr'):\n",
    "            abbr_list.append(abbr)\n",
    "        for x in abbr_list:\n",
    "            temp = x.contents[0].contents[0].contents\n",
    "            if isinstance(temp[0], NavigableString):\n",
    "                slang = ''.join(temp)\n",
    "                slang = slang.encode('ascii', 'ignore')\n",
    "                slang = slang.split(' :')[0]\n",
    "                meaning = x['title']\n",
    "                slang_dict[slang] = meaning\n",
    "        url = url[:-2]\n",
    "    return slang_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def removeRepeatedSequence(word):\n",
    "    pattern = re.compile(r'(.)\\1{1,}',re.DOTALL)\n",
    "    return pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getWordVector(tweet):\n",
    "    wordVector = []\n",
    "    with open('StopWords.txt', 'r') as f:\n",
    "        stopWords = f.read().split('\\n')[1:-1]\n",
    "        f.close()\n",
    "    words = tweet.split()\n",
    "    for word in words:\n",
    "        word = removeRepeatedSequence(word)\n",
    "        word = word.strip('\\'\"?,.')\n",
    "        s = re.search(r'^[a-zA-Z][a-zA-Z0-9]*$', word)\n",
    "        if word.lower() in stopWords or s is None:\n",
    "            continue\n",
    "        else:\n",
    "            wordVector.append(word.lower())\n",
    "    return wordVector\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preProcessTweet(tweet, emoji_dict):\n",
    "    tweet = tweet.lower()\n",
    "    negative_words = ['not','no','never','n\\'t','cannot','isn\\'t','isnt']\n",
    "    tweet = re.sub(r'{(\\s)*link(\\s)*}','U',tweet)\n",
    "    tweet = re.sub(r'@(\\s)*(\\w)+','@target',tweet)\n",
    "    tweet = re.sub(r'[\\s]+',' ',tweet)\n",
    "    tweet = re.sub(r'#(\\s)*(\\w)+',r'\\2',tweet)\n",
    "    tweet = tweet.strip('\\'\"')\n",
    "    \n",
    "    tokeniser = TweetTokenizer() #Have to use something else as it is seperating the emoticon\n",
    "    tweet_tokens = tokeniser.tokenize(tweet)\n",
    "    for x in range(len(tweet_tokens)):\n",
    "        token = tweet_tokens[x]\n",
    "        if len(token) == 1 and ord(token) not in range(0,128):\n",
    "            continue\n",
    "        token = str(token)\n",
    "        if token in negative_words:\n",
    "            token = 'NOT'\n",
    "        if token in emoji_dict:\n",
    "            if emoji_dict[token] <= -3:\n",
    "                token = 'EXTREMELY-NEGATIVE'\n",
    "            if emoji_dict[token] > -3 and emoji_dict[token] < 0:\n",
    "                token = 'NEGATIVE'\n",
    "            if emoji_dict[token] == 0:\n",
    "                token = 'NEUTRAL'\n",
    "            if emoji_dict[token] > 0 and emoji_dict[token] < 3:\n",
    "                token = 'POSITIVE'\n",
    "            if emoji_dict[token] >= 3:\n",
    "                token = 'EXTREMELY-POSITIVE'\n",
    "        tweet_tokens[x] = token\n",
    "    tweet = ' '.join(tweet_tokens)\n",
    "    \n",
    "    return tweet\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def buildUnigramVector(tweets, wordList):\n",
    "    \n",
    "    features = []\n",
    "    sentiment = []\n",
    "    for tweet in tweets:\n",
    "        sentiment = 0\n",
    "        wordMap = {}\n",
    "        for word in sorted(wordList):\n",
    "            wordMap[word] = 0\n",
    "        \n",
    "        tweetWords = tweet[0]\n",
    "        for w in tweetWords:\n",
    "            w = removeRepeatedSequence(w)\n",
    "            w = w.strip('\\'\".,?')\n",
    "            if w in wordMap:\n",
    "                wordMap[w] = 1\n",
    "            val = wordMap.values()\n",
    "            features.append(val)\n",
    "            sentiment.append(tweet[1])\n",
    "    unigramVector = {'unigram_vector':features, 'sentiment': sentiment}\n",
    "    return unigramVector\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentiFeatures():\n",
    "    slang_dict = DictionaryBuilder()\n",
    "    negative_word_list =['not','no','never','n\\'t','cannot']\n",
    "    train = pd.read_csv(\"train.csv\")\n",
    "\n",
    "    with open('StopWords.txt', 'r') as f:\n",
    "        stopWords = f.read().split('\\n')[1:-1]\n",
    "        f.close()\n",
    "    \n",
    "    emoticon_dict = collections.defaultdict(int)\n",
    "    reader = csv.reader(open('emoticons.csv', 'r'), skipinitialspace=True)\n",
    "    for row in reader:\n",
    "        emoticon_dict[row[0].strip()] = int(row[1])\n",
    "  \n",
    "    prev_tag = ''\n",
    "    tree_list =[]\n",
    "    feature_vector =[]\n",
    "    tweets_tagged =[]\n",
    "    tokenizer = TweetTokenizer()\n",
    "    for line in reader:\n",
    "        tweets_tagged.append(line)\n",
    "\n",
    "    featureList =[]\n",
    "\n",
    "    tweets =[]\n",
    "    for i in range(len(train)):\n",
    "        tweet = train['tweet'][i]\n",
    "        sentiment = train['sentiment'][i]\n",
    "        \n",
    "        tweet = preProcessTweet(tweet, emoticon_dict)\n",
    "        f1, f2, f3, f4, f5, f6, f7, f9, f10, f11 = 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "        tweet_tagged = tweets_tagged[i]\n",
    "        \n",
    "        tokens = tokenizer.tokenize(tweet)\n",
    "        tagged_dict = collections.defaultdict(str)\n",
    "        \n",
    "        for word in tweet_tagged:\n",
    "            if len(word.split('_') == 2):\n",
    "                word, tag = word.split('_')\n",
    "                tagged_dict[word] = tag\n",
    "                \n",
    "        for j in range(len(tokens)):\n",
    "            token = tokens[j]\n",
    "            if token == '!':\n",
    "                f11=1.0\n",
    "                if prev_tag == 'polar':\n",
    "                    f4+=1.0\n",
    "                continue\n",
    "                \n",
    "            if len(token) == 1 and ord(token) not in range(1, 128):\n",
    "                continue\n",
    "            else:\n",
    "                token = str(token)\n",
    "                \n",
    "            if token == '@target':\n",
    "                token = '||T||'\n",
    "                f7+=1.0\n",
    "                prev_tag = 'target'\n",
    "                continue\n",
    "            if token == 'U':\n",
    "                token = '||U||'\n",
    "                f7+=1.0\n",
    "                prev_tag = 'link'\n",
    "                continue\n",
    "            if token.startswith('#'):\n",
    "                f4+=1.0\n",
    "                f7+=1.0\n",
    "                prev_tag = 'hash'\n",
    "                continue\n",
    "            \n",
    "            if not (re.match(r'a-zA-Z', token) != None or token != removeRepeatedSequence(token)):\n",
    "                prev_tag = 'emp'\n",
    "                continue\n",
    "                \n",
    "            if token in stopWords:\n",
    "                prev_tag = 'stop'\n",
    "                continue\n",
    "                \n",
    "            if token.lower() in slang_dict:\n",
    "                prev_tag = 'slang'\n",
    "                token = slang_dict[token.lower()]\n",
    "                f6+=1.0\n",
    "                continue\n",
    "            \n",
    "            if token == 'POSITIVE' or token == 'EXTREMENLY-POSITIVE' or token == 'NEGATIVE' or token == 'EXTREMELY-NEGATIVE':\n",
    "                f3+=1.0\n",
    "                continue\n",
    "            \n",
    "            if token == 'NOT':\n",
    "                f2+=1.0\n",
    "                continue\n",
    "                \n",
    "            synsets = swn.senti_synsets(token)\n",
    "            if len(synsets) > 0:\n",
    "                \n",
    "                score = max([synset.pos_score(), synset.neg_score(), synset.obj_score()])\n",
    "                if synset.pos_score() == synset.neg_score() or synset.obj_score() == score:\n",
    "                    score = 0.0\n",
    "                if synset.pos_score() < synset.neg_score():\n",
    "                    score = -score\n",
    "                \n",
    "                if score != 0.0:\n",
    "                    prev_tag = 'polar'\n",
    "                    f2+=1.0\n",
    "                    if token.isupper() == True:\n",
    "                        f4+=1.0\n",
    "                        f11=1.0\n",
    "                    \n",
    "                    f8 = {}\n",
    "                    if token in tagged_dict and (tagged_dict[token] == 'JJ' or tagged_dict[token] == 'RB' or tagged_dict[token] == 'VB' or tagged_dict[token] == 'NN'):            \n",
    "                        f1+=1.0\n",
    "                        f8[tagged_dict[token]]+=score\n",
    "                    else:\n",
    "                        f9+=score\n",
    "                else:\n",
    "                    prev_tag = 'non-polar'\n",
    "                    if token in tagged_dict and (tagged_dict[token] == 'JJ' or tagged_dict[token] == 'RB' or tagged_dict[token] == 'VB' or tagged_dict[token] == 'NN'):            \n",
    "                        f5 += 1.0\n",
    "                    if token.isupper():\n",
    "                        f10 += 1.0\n",
    "                        f11 = 1.0\n",
    "                continue\n",
    "            else:\n",
    "                prev_tag = 'non-polar'\n",
    "                if token in tagged_dict and (tagged_dict[token] == 'JJ' or tagged_dict[token] == 'RB' or tagged_dict[token] == 'VB' or tagged_dict[token] == 'NN'):            \n",
    "                    f5 += 1.0\n",
    "                if token.isupper():\n",
    "                    f10 += 1.0\n",
    "                    f11 = 1.0               \n",
    "                continue\n",
    "        tweet = ' '.join(tokens)\n",
    "        word_vector = getWordVector(tweet)\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tag import StanfordPOSTagger\n",
    "st = StanfordPOSTagger('english-bidirectional-distsim.tagger') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
